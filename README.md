# Spot-Meta-Teleop  
**Meta Quest 3 âœ• Boston Dynamics Spot SDK 5.0**  
Remote-operate Spotâ€™s base, arm and gripper with natural 6-DoF hand motions.

---

## âœ¨  Key Features
| Capability | Details |
|------------|---------|
| **Full-body tele-operation** | Drive base with left-right joystick, servo arm & claw with right controller. |
| **Anchor-and-delta control** | Hold grip â†’ anchor pose; move hand â†’ end-effector follows, 30 Hz. |
| **Safe start-up / shutdown** | Script auto-acquires lease, clears Keepalive & Estop, undocks, stands, and power-offs cleanly. |
| **No ROS, no Unity** | Pure Python 3.10 on top of official `bosdyn-client == 5.0.0`. |
| **Quest 3 tracking pipeline** | Uses [`OculusReader`](https://github.com/rail-berkeley/oculus_reader) for sub-10 ms pose streaming. |

---

## Controller Key Map

ToDo

---

## ðŸ›   Prerequisites

| Component | Tested version |
|-----------|----------------|
| Python | **3.10** (conda recommended) |
| Spot SDK wheels | `bosdyn-client==5.0.0` `bosdyn-mission==5.0.0` `bosdyn-choreography-client==5.0.0` `bosdyn-orbit==5.0.0` |
| Meta Quest 3 | v66 firmware, **Developer Mode ON** |
| Network | Robot â†” PC â†” Quest on same low-latency subnet (â‰¤ 2 ms) |

> **Safety** â€“ Keep the OEM E-Stop within reach. Never run tele-op unattended.

---

## ðŸ“¦ Installation

```bash
# 1. conda env
conda create -n spot python=3.10 -y
conda activate spot

# 2. clone
git clone https://github.com/mnakash/spot-meta-teleop.git
cd spot-meta-teleop

# 3. Install requirements
sudo apt update
sudo apt install mpg123
python -m pip install -r requirements.txt
```

## Setup Environment Variables
Open your .bashrc file in a text editor by `nano ~/.bashrc`.
Scroll to the bottom and add the following lines:
```
# Spot VR Teleop environment variables
export SPOT_ROBOT_IP=<robot_ip>
export BOSDYN_CLIENT_USERNAME=<username>
export BOSDYN_CLIENT_PASSWORD=<password>
```
Save and exit (Ctrl + O, Enter, then Ctrl + X in nano)

Apply changes immediately:
`source ~/.bashrc`

Verify they are set:
```
echo $SPOT_ROBOT_IP
echo $BOSDYN_CLIENT_USERNAME
echo $BOSDYN_CLIENT_PASSWORD
```


## Setup Meta controller
Follow guidelines here in [`OculusReader`](https://github.com/rail-berkeley/oculus_reader) repo for setting up Meta Quest Controller over WiFi or USB.

## How to run
1. Make sure to be able to run [reader.py](reader.py) before controlling spot.
2. Run `python3 teleop_spot.py` to start controlling spot.


## .npz file structure

Each savedâ€¯.npzâ€¯contains one â€œsessionâ€ dictionary with the following keys:


| Key                   | Shape                 | Description                                                                        |
| --------------------- | --------------------- | ---------------------------------------------------------------------------------- |
| **images_0**          | `(N,H,W,C)` (dtype=object) | Eye-in-hand RGB camera frames; each element is an `HxWÃ—3` `uint8` ndarray. |
| **images_1**          | `(N,H,W,C)` (dtype=object) | Agentview RGB camera stream (`images_x`, x>=1) when external camera(s) are enabled. |
| **arm\_joint\_names** | `(J,)` (dtype=`<Uâ€¦`)  | The J jointâ€names (strings) for all recorded â€œarm0.\*â€ joints.                     |
| **arm\_q**            | `(N, J)` (float32)    | Joint positions at each timestep.                                                  |
| **arm\_dq**           | `(N, J)` (float32)    | Joint velocities at each timestep.                                                 |
| **ee\_pose**          | `(N, 7)` (float32)    | Endâ€‘effector pose in body frame: `[tx,ty,tz,qx,qy,qz,qw]`.                         |
| **vision\_in\_body**  | `(N, 7)` (float32)    | Visionâ€‘frame origin expressed in body frame (same format as `ee_pose`).            |
| **body\_vel**         | `(N, 6)` (float32)    | Body linear & angular velocity in vision frame: `[vx,vy,vz,wx,wy,wz]`.             |
| **gripper**           | `(N, 1)` (float32)    | Gripper opening percentage.                                                        |
| **ee\_force**         | `(N, 3)` (float32)    | Estimated endâ€‘effector force vector in hand.                                       |
| **images\_0\_depth**  | `(N,H,W)` (dtype=object) | Eye-in-hand depth frames (`HxW` `uint16`). No colorization is applied. |
| **images\_1\_depth**  | `(N,H,W)` (dtype=object) | Agentview depth stream (`images_x_depth`, x>=1) when external camera(s) are enabled. |
| **t**                 | `(N, 1)` (float64)    | Timestamp for each capture, in seconds (including fractional).                     |

## ðŸ“‚ Dataset Structure (.h5)
The dataset is organized in the following hierarchy:

```text
arm_joint_names                (shape=(7,), dtype=|S8)
data/
â”œâ”€â”€ demo_0/
|    â”œâ”€â”€ actions                    (shape=(N, 10), dtype=float32)
|    â””â”€â”€ obs/
|        â”œâ”€â”€ arm_dq                 (shape=(N, 7), dtype=float32)
|        â”œâ”€â”€ arm_q                  (shape=(N, 7), dtype=float32)
|        â”œâ”€â”€ body_vel               (shape=(N, 6), dtype=float32)
|        â”œâ”€â”€ ee_force               (shape=(N, 3), dtype=float32)
|        â”œâ”€â”€ eef_pos                (shape=(N, 3), dtype=float32)
|        â”œâ”€â”€ eef_quat               (shape=(N, 4), dtype=float32)
|        â”œâ”€â”€ gripper                (shape=(N, 1), dtype=float32)
|        â”œâ”€â”€ images_0               (shape=(N, 240, 320, 3), dtype=uint8)
|        â”œâ”€â”€ images_0_depth         (shape=(N, 240, 320), dtype=uint16)
|        â”œâ”€â”€ images_1               (shape=(N, 240, 320, 3), dtype=uint8)
|        â”œâ”€â”€ images_1_depth         (shape=(N, 240, 320), dtype=uint16)
|        â”œâ”€â”€ t                      (shape=(N, 1), dtype=float32)
|        â””â”€â”€ vision_in_body         (shape=(N, 7), dtype=float32)
â”œâ”€â”€ demo_1/
...
â”œâ”€â”€ demo_N/
